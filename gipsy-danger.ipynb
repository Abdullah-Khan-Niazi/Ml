{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "MODEL_REPO = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "MODEL_FILENAME = \"deepseek-coder-1.3b-base.Q4_K_M.gguf\"\n",
    "MODEL_DIR = os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), \"models\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    hf_hub_download(\n",
    "        repo_id=MODEL_REPO,\n",
    "        filename=MODEL_FILENAME,\n",
    "        local_dir=MODEL_DIR,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=4096,\n",
    "    n_threads=os.cpu_count(),\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def ask_llm(prompt: str, max_tokens: int = 512) -> str:\n",
    "    output = llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        stop=[\"\\n\\n\", \"\\n#\", \"\\nclass\", \"\\ndef\"]\n",
    "    )\n",
    "    return output[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ad04a",
   "metadata": {},
   "source": [
    "# üßë‚Äçüíª Offline DeepSeek Coder LLM in Jupyter\n",
    "\n",
    "## üì¶ Installation & Setup\n",
    "\n",
    "1. **Install dependencies** (run the first code cell):\n",
    "   ```python\n",
    "   !pip install llama-cpp-python huggingface-hub\n",
    "   ```\n",
    "2. **First run:** The model will be downloaded automatically (about 2GB, one time only).\n",
    "3. **After download:** The notebook works fully offline for coding questions.\n",
    "\n",
    "## üóÇÔ∏è File Structure\n",
    "\n",
    "- This notebook (Offline DeepSeek LLM.ipynb)\n",
    "- `models/` folder (created automatically)\n",
    "    - `deepseek-coder-1.3b-base.Q4_K_M.gguf` (downloaded model)\n",
    "\n",
    "## üß† Usage\n",
    "\n",
    "- Use the `ask_llm(prompt)` function to ask coding questions.\n",
    "- Example:\n",
    "  ```python\n",
    "  ask_llm(\"Write generic sklearn code for logistic regression\")\n",
    "  ```\n",
    "- You can change the prompt as needed.\n",
    "\n",
    "## ‚öôÔ∏è Model Details\n",
    "- Model: DeepSeek Coder 1.3B (GGUF, quantized)\n",
    "- Context: 4096 tokens\n",
    "- CPU only, deterministic, stable\n",
    "- No internet required after first download\n",
    "\n",
    "---\n",
    "\n",
    "**Start below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64590c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_llm(\"Write generic sklearn code for logistic regression\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
